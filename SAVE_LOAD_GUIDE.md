# í† í¬ë‚˜ì´ì € ì €ì¥/ë¡œë“œ ê¸°ëŠ¥ ì‚¬ìš© ê°€ì´ë“œ

## ì£¼ìš” ê¸°ëŠ¥

### 1. ìë™ ì €ì¥
- BPE í† í¬ë‚˜ì´ì € (ì˜ì–´/ë…ì¼ì–´) ì €ì¥
- ì–´íœ˜ì‚¬ì „ ì €ì¥
- ë©”íƒ€ë°ì´í„° ì €ì¥ (vocab í¬ê¸°, íŠ¹ìˆ˜ í† í° ì¸ë±ìŠ¤ ë“±)

### 2. ë¡œë“œ ë° ìŠ¤í‚µ
- ì €ì¥ëœ í† í¬ë‚˜ì´ì €/ì–´íœ˜ë¥¼ ë¡œë“œí•˜ì—¬ ì¬í•™ìŠµ ë°©ì§€
- í† í¬ë‚˜ì´ì € í›ˆë ¨ ì‹œê°„ ì ˆì•½ (8-12ì‹œê°„ â†’ 0ì´ˆ)
- ë°”ë¡œ ëª¨ë¸ í•™ìŠµ ì‹œì‘ ê°€ëŠ¥

## ì‚¬ìš© ë°©ë²•

### ì‹œë‚˜ë¦¬ì˜¤ 1: ì²˜ìŒ ì‹¤í–‰ (í† í¬ë‚˜ì´ì € í•™ìŠµ + ì €ì¥)

```bash
# ê¸°ë³¸ ì‹¤í–‰ - artifacts/ ë””ë ‰í† ë¦¬ì— ìë™ ì €ì¥
python demo_wmt14_saveable.py

# ì €ì¥ ê²½ë¡œ ì§€ì •
python demo_wmt14_saveable.py --save_dir my_artifacts
```

**ì €ì¥ë˜ëŠ” íŒŒì¼:**
```
artifacts/
â”œâ”€â”€ tokenizer_en.pkl      # ì˜ì–´ BPE í† í¬ë‚˜ì´ì €
â”œâ”€â”€ tokenizer_de.pkl      # ë…ì¼ì–´ BPE í† í¬ë‚˜ì´ì €
â”œâ”€â”€ vocab.pkl             # ì–´íœ˜ì‚¬ì „
â””â”€â”€ metadata.json         # ë©”íƒ€ë°ì´í„°
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„:**
- ë°ì´í„° ë¡œë“œ: ~2ë¶„
- í† í¬ë‚˜ì´ì € í›ˆë ¨: 8-12ì‹œê°„ â¬…ï¸ í•œ ë²ˆë§Œ!
- ì–´íœ˜ êµ¬ì¶•: ~5ë¶„
- ì €ì¥: ~10ì´ˆ
- **ì´: ì•½ 8-12ì‹œê°„**

### ì‹œë‚˜ë¦¬ì˜¤ 2: ì €ì¥ëœ ê²°ê³¼ë¬¼ë¡œ í•™ìŠµ (í† í¬ë‚˜ì´ì € ìŠ¤í‚µ)

```bash
# ì €ì¥ëœ ê²°ê³¼ë¬¼ ë¡œë“œí•˜ì—¬ ë°”ë¡œ ëª¨ë¸ í•™ìŠµ
python demo_wmt14_saveable.py --load_dir artifacts

# ë‹¤ë¥¸ ê²½ë¡œì—ì„œ ë¡œë“œ
python demo_wmt14_saveable.py --load_dir my_artifacts
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„:**
- ë°ì´í„° ë¡œë“œ: ~2ë¶„
- í† í¬ë‚˜ì´ì €/ì–´íœ˜ ë¡œë“œ: **~5ì´ˆ** â¬…ï¸ ì—„ì²­ ë¹ ë¦„!
- Iterator ìƒì„±: ~30ì´ˆ
- ëª¨ë¸ í•™ìŠµ: (ì—í­ ìˆ˜ì— ë”°ë¼)
- **ì´: ~3ë¶„ + í•™ìŠµ ì‹œê°„**

### ì‹œë‚˜ë¦¬ì˜¤ 3: ë‹¤ì–‘í•œ ì„¤ì •ìœ¼ë¡œ ì‹¤í—˜

```bash
# ì‘ì€ vocabìœ¼ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python demo_wmt14_saveable.py --vocab_size 10000 --save_dir artifacts_10k

# ë‚˜ì¤‘ì— ì´ ê²°ê³¼ë¬¼ë¡œ ì—¬ëŸ¬ ì‹¤í—˜
python demo_wmt14_saveable.py --load_dir artifacts_10k --epochs 50
python demo_wmt14_saveable.py --load_dir artifacts_10k --epochs 100 --max_tokens 15000
```

## ëª…ë ¹í–‰ ì˜µì…˜

### í•„ìˆ˜ ì˜µì…˜

#### `--load_dir` (ì €ì¥ëœ ê²°ê³¼ë¬¼ ë¡œë“œ)
```bash
python demo_wmt14_saveable.py --load_dir artifacts
```
- í† í¬ë‚˜ì´ì € í›ˆë ¨ ë° ì–´íœ˜ êµ¬ì¶• ìŠ¤í‚µ
- ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œ
- ì—†ìœ¼ë©´ ì²˜ìŒë¶€í„° í•™ìŠµ

#### `--save_dir` (ì €ì¥ ê²½ë¡œ ì§€ì •)
```bash
python demo_wmt14_saveable.py --save_dir my_output
```
- ê¸°ë³¸ê°’: `artifacts`
- í† í¬ë‚˜ì´ì €ì™€ ì–´íœ˜ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬

### í•™ìŠµ ì˜µì…˜

#### `--epochs` (ì—í­ ìˆ˜)
```bash
python demo_wmt14_saveable.py --epochs 50
```
- ê¸°ë³¸ê°’: 100
- í•™ìŠµ ì—í­ ìˆ˜

#### `--max_tokens` (ë°°ì¹˜ë‹¹ í† í° ìˆ˜)
```bash
python demo_wmt14_saveable.py --max_tokens 15000
```
- ê¸°ë³¸ê°’: 25000 (ë…¼ë¬¸ ì¤€ìˆ˜)
- GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •

#### `--vocab_size` (ì–´íœ˜ í¬ê¸°)
```bash
python demo_wmt14_saveable.py --vocab_size 10000
```
- ê¸°ë³¸ê°’: 37000 (ë…¼ë¬¸ ì¤€ìˆ˜)
- ì‘ì„ìˆ˜ë¡ ë¹ ë¥´ê²Œ í•™ìŠµ

#### `--checkpoint_dir` (ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ)
```bash
python demo_wmt14_saveable.py --checkpoint_dir my_checkpoints
```
- ê¸°ë³¸ê°’: `checkpoints`
- ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬

## ì‹¤ì „ ì›Œí¬í”Œë¡œìš°

### Step 1: í† í¬ë‚˜ì´ì € í•œ ë²ˆë§Œ í•™ìŠµ (8-12ì‹œê°„)

```bash
# ë…¼ë¬¸ ì„¤ì •ìœ¼ë¡œ í† í¬ë‚˜ì´ì € í•™ìŠµ
python demo_wmt14_saveable.py \
    --vocab_size 37000 \
    --save_dir artifacts_37k \
    --epochs 1
```

- `--epochs 1`: 1 ì—í­ë§Œ í•™ìŠµí•˜ê³  ì¤‘ë‹¨ (Ctrl+C)
- í† í¬ë‚˜ì´ì €ì™€ ì–´íœ˜ë§Œ ì €ì¥í•˜ë©´ ë¨

### Step 2: ë‹¤ì–‘í•œ ì‹¤í—˜ ë°˜ë³µ (ì¦‰ì‹œ ì‹œì‘!)

```bash
# ì‹¤í—˜ 1: Base ëª¨ë¸ (ë…¼ë¬¸ ì„¤ì •)
python demo_wmt14_saveable.py \
    --load_dir artifacts_37k \
    --epochs 100 \
    --max_tokens 25000 \
    --checkpoint_dir exp1_base

# ì‹¤í—˜ 2: ì‘ì€ ë°°ì¹˜
python demo_wmt14_saveable.py \
    --load_dir artifacts_37k \
    --epochs 100 \
    --max_tokens 15000 \
    --checkpoint_dir exp2_small_batch

# ì‹¤í—˜ 3: ê¸´ í•™ìŠµ
python demo_wmt14_saveable.py \
    --load_dir artifacts_37k \
    --epochs 200 \
    --checkpoint_dir exp3_long
```

ëª¨ë“  ì‹¤í—˜ì´ **ì¦‰ì‹œ ì‹œì‘**ë©ë‹ˆë‹¤!

## ì¶œë ¥ ì˜ˆì‹œ

### ì²˜ìŒ ì‹¤í–‰ (ì €ì¥)

```
================================================================================
                    WMT14 Transformer Training
================================================================================

Configuration:
  Load directory: None (train from scratch)
  Save directory: artifacts
  Epochs: 100
  Max tokens per batch: 25,000
  Vocabulary size: 37,000
  Checkpoint directory: checkpoints

================================================================================
1. ë°ì´í„°ì…‹ ë¡œë“œ
================================================================================

Loading WMT14 dataset...
...

================================================================================
2. BPE í† í¬ë‚˜ì´ì € í›ˆë ¨
================================================================================

Training English BPE Tokenizer
...
[Step 4/4] Performing BPE merges...
  Merges: 36,687/36,687 (100.0%) | Vocab: 37,000/37,000 | Done!
...

================================================================================
ì €ì¥ ì¤‘...
================================================================================

Saving tokenizer to artifacts/tokenizer_en.pkl...
âœ“ Tokenizer saved
Saving tokenizer to artifacts/tokenizer_de.pkl...
âœ“ Tokenizer saved
Saving vocabulary to artifacts/vocab.pkl...
âœ“ Vocabulary saved

âœ“ All artifacts saved to: artifacts
  - tokenizer_en.pkl
  - tokenizer_de.pkl
  - vocab.pkl
  - metadata.json
```

### ë¡œë“œí•˜ì—¬ ì‹¤í–‰

```
================================================================================
                    WMT14 Transformer Training
================================================================================

Configuration:
  Load directory: artifacts
  Save directory: artifacts
  Epochs: 100
  Max tokens per batch: 25,000
  Vocabulary size: 37,000
  Checkpoint directory: checkpoints

================================================================================
1. ë°ì´í„°ì…‹ ë¡œë“œ
================================================================================
...

================================================================================
ê¸°ì¡´ ê²°ê³¼ë¬¼ ë¡œë“œ ì¤‘...
================================================================================

Metadata:
  Created: 2026-01-30 14:23:45
  Vocab size: 37,000
  Shared vocab: True

Loading tokenizer from artifacts/tokenizer_en.pkl...
âœ“ Tokenizer loaded (vocab size: 37,000)
Loading tokenizer from artifacts/tokenizer_de.pkl...
âœ“ Tokenizer loaded (vocab size: 37,000)

Loading vocabulary from artifacts/vocab.pkl...
âœ“ Vocabulary loaded
  Source vocab size: 37,000
  Target vocab size: 37,000
  Shared: True

âœ“ All artifacts loaded from: artifacts

[í† í¬ë‚˜ì´ì € í›ˆë ¨ ì™„ì „íˆ ìŠ¤í‚µ!]

================================================================================
ì–´íœ˜ ì •ë³´
================================================================================
...
```

## ì €ì¥ íŒŒì¼ êµ¬ì¡°

### artifacts/ ë””ë ‰í† ë¦¬

```
artifacts/
â”œâ”€â”€ tokenizer_en.pkl       # 21 MB - ì˜ì–´ BPE í† í¬ë‚˜ì´ì €
â”‚   â”œâ”€â”€ vocab_size
â”‚   â”œâ”€â”€ vocab (list)
â”‚   â”œâ”€â”€ merges (dict)
â”‚   â””â”€â”€ base_tokenizer
â”‚
â”œâ”€â”€ tokenizer_de.pkl       # 22 MB - ë…ì¼ì–´ BPE í† í¬ë‚˜ì´ì €
â”‚   â””â”€â”€ (ë™ì¼ êµ¬ì¡°)
â”‚
â”œâ”€â”€ vocab.pkl              # 15 MB - ì–´íœ˜ì‚¬ì „
â”‚   â”œâ”€â”€ source_stoi (dict)
â”‚   â”œâ”€â”€ source_itos (dict)
â”‚   â”œâ”€â”€ target_stoi (dict)
â”‚   â”œâ”€â”€ target_itos (dict)
â”‚   â””â”€â”€ shared (bool)
â”‚
â””â”€â”€ metadata.json          # 1 KB - ë©”íƒ€ë°ì´í„°
    â”œâ”€â”€ vocab_size
    â”œâ”€â”€ shared_vocab
    â”œâ”€â”€ src_pad_idx
    â”œâ”€â”€ trg_pad_idx
    â”œâ”€â”€ trg_sos_idx
    â””â”€â”€ timestamp
```

**ì´ ìš©ëŸ‰:** ì•½ 60 MB

### checkpoints/ ë””ë ‰í† ë¦¬

```
checkpoints/
â”œâ”€â”€ model_epoch_10.pt      # 250 MB - 10 ì—í­
â”œâ”€â”€ model_epoch_20.pt      # 250 MB - 20 ì—í­
â”œâ”€â”€ model_epoch_30.pt      # 250 MB - 30 ì—í­
â”œâ”€â”€ ...
â””â”€â”€ model_final.pt         # 250 MB - ìµœì¢… ëª¨ë¸
```

ê° ì²´í¬í¬ì¸íŠ¸ í¬í•¨ ë‚´ìš©:
- model_state_dict
- optimizer_state_dict
- scheduler_state_dict
- epoch, train_loss, val_loss

## ê³ ê¸‰ ì‚¬ìš©ë²•

### 1. ë‹¤ë¥¸ vocab í¬ê¸°ë¡œ ì—¬ëŸ¬ ë²„ì „ ì €ì¥

```bash
# Vocab 10K
python demo_wmt14_saveable.py --vocab_size 10000 --save_dir artifacts_10k --epochs 1

# Vocab 20K
python demo_wmt14_saveable.py --vocab_size 20000 --save_dir artifacts_20k --epochs 1

# Vocab 37K (ë…¼ë¬¸)
python demo_wmt14_saveable.py --vocab_size 37000 --save_dir artifacts_37k --epochs 1
```

ë‚˜ì¤‘ì— ì›í•˜ëŠ” ë²„ì „ ì„ íƒ:
```bash
python demo_wmt14_saveable.py --load_dir artifacts_10k --epochs 100
```

### 2. ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ

```python
# ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
checkpoint = torch.load('checkpoints/model_epoch_50.pt')

model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

# 50 ì—í­ë¶€í„° ì¬ê°œ
for epoch in range(checkpoint['epoch'], 100):
    # ...
```

### 3. í† í¬ë‚˜ì´ì €ë§Œ êµì²´í•˜ì—¬ ì‹¤í—˜

```python
# tokenizer_custom.py
from tokenizer_with_progress import BPETokenizer

# ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì €ë¡œ í›ˆë ¨
custom_tokenizer = BPETokenizer(vocab_size=50000)
custom_tokenizer.train(corpus)

# ì €ì¥
import pickle
with open('artifacts/tokenizer_en.pkl', 'wb') as f:
    pickle.dump(custom_tokenizer, f)
```

## ì„±ëŠ¥ ë¹„êµ

### í† í¬ë‚˜ì´ì € í•™ìŠµ í¬í•¨ (ê¸°ì¡´)

| ë‹¨ê³„ | ì†Œìš” ì‹œê°„ |
|------|----------|
| ë°ì´í„° ë¡œë“œ | 2ë¶„ |
| **í† í¬ë‚˜ì´ì € í›ˆë ¨ (EN)** | **4-6ì‹œê°„** |
| **í† í¬ë‚˜ì´ì € í›ˆë ¨ (DE)** | **4-6ì‹œê°„** |
| ì–´íœ˜ êµ¬ì¶• | 5ë¶„ |
| Iterator ìƒì„± | 30ì´ˆ |
| **ì´ (í•™ìŠµ ì „)** | **8-12ì‹œê°„** |

### ì €ì¥ëœ ê²°ê³¼ë¬¼ ë¡œë“œ (ê°œì„ )

| ë‹¨ê³„ | ì†Œìš” ì‹œê°„ |
|------|----------|
| ë°ì´í„° ë¡œë“œ | 2ë¶„ |
| **í† í¬ë‚˜ì´ì € ë¡œë“œ** | **5ì´ˆ** âš¡ |
| Iterator ìƒì„± | 30ì´ˆ |
| **ì´ (í•™ìŠµ ì „)** | **3ë¶„** âš¡âš¡âš¡ |

**ì‹œê°„ ì ˆì•½: ì•½ 99%** (8-12ì‹œê°„ â†’ 3ë¶„)

## ë¬¸ì œ í•´ê²°

### Q1: "FileNotFoundError: Missing required files"

**ì›ì¸:** ì§€ì •ëœ ë””ë ‰í† ë¦¬ì— í•„ìˆ˜ íŒŒì¼ì´ ì—†ìŒ

**í•´ê²°:**
```bash
# ë””ë ‰í† ë¦¬ ë‚´ìš© í™•ì¸
ls -lh artifacts/

# í•„ìˆ˜ íŒŒì¼: tokenizer_en.pkl, tokenizer_de.pkl, vocab.pkl, metadata.json
# ì—†ìœ¼ë©´ --load_dir ì—†ì´ ë‹¤ì‹œ ì‹¤í–‰
python demo_wmt14_saveable.py --save_dir artifacts
```

### Q2: "Vocab sizeê°€ ì˜ˆìƒê³¼ ë‹¤ë¦„"

**ì›ì¸:** ë‹¤ë¥¸ ì„¤ì •ìœ¼ë¡œ ì €ì¥ëœ ê²°ê³¼ë¬¼ ì‚¬ìš©

**í™•ì¸:**
```bash
# metadata.json í™•ì¸
cat artifacts/metadata.json

{
  "vocab_size": 10000,  # â† í™•ì¸
  "shared_vocab": true,
  ...
}
```

**í•´ê²°:** ì˜¬ë°”ë¥¸ ë””ë ‰í† ë¦¬ ì‚¬ìš© ë˜ëŠ” ì¬í•™ìŠµ

### Q3: í† í¬ë‚˜ì´ì € ë¡œë“œ í›„ ì„±ëŠ¥ì´ ì´ìƒí•¨

**ì›ì¸:** ë‹¤ë¥¸ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµëœ í† í¬ë‚˜ì´ì € ì‚¬ìš©

**í•´ê²°:** ë™ì¼í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµëœ í† í¬ë‚˜ì´ì € ì‚¬ìš©

### Q4: ì €ì¥ íŒŒì¼ì´ ë„ˆë¬´ í¼

**ì •ìƒ:** 
- tokenizer_*.pkl: ì•½ 20-25 MB ê°
- vocab.pkl: ì•½ 15 MB
- ì´ ì•½ 60 MB

**ì¤„ì´ëŠ” ë°©ë²•:**
```bash
# ë” ì‘ì€ vocab ì‚¬ìš©
python demo_wmt14_saveable.py --vocab_size 10000
```

## ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. ì²˜ìŒì—ëŠ” ì‘ì€ vocabìœ¼ë¡œ í…ŒìŠ¤íŠ¸

```bash
# Step 1: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (2-3ì‹œê°„)
python demo_wmt14_saveable.py \
    --vocab_size 10000 \
    --save_dir artifacts_10k_test \
    --epochs 5

# Step 2: ì˜ ì‘ë™í•˜ë©´ í° vocabìœ¼ë¡œ (8-12ì‹œê°„)
python demo_wmt14_saveable.py \
    --vocab_size 37000 \
    --save_dir artifacts_37k_final \
    --epochs 1
```

### 2. í† í¬ë‚˜ì´ì €ë§Œ ë¨¼ì € í•™ìŠµ

```bash
# í† í¬ë‚˜ì´ì €ë§Œ í•™ìŠµ (Ctrl+Cë¡œ ì¤‘ë‹¨)
python demo_wmt14_saveable.py \
    --vocab_size 37000 \
    --save_dir artifacts_37k \
    --epochs 1

# Press Enter to start training í›„ Ctrl+C
# í† í¬ë‚˜ì´ì €ì™€ ì–´íœ˜ëŠ” ì´ë¯¸ ì €ì¥ë¨!
```

### 3. ì—¬ëŸ¬ ì‹¤í—˜ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
experiments/
â”œâ”€â”€ artifacts_37k/          # ë…¼ë¬¸ ì„¤ì •
â”œâ”€â”€ artifacts_20k/          # ì¤‘ê°„ í¬ê¸°
â”œâ”€â”€ artifacts_10k/          # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
â”‚
â”œâ”€â”€ exp1_baseline/          # ì‹¤í—˜ 1
â”‚   â””â”€â”€ checkpoints/
â”œâ”€â”€ exp2_lr_tuning/         # ì‹¤í—˜ 2
â”‚   â””â”€â”€ checkpoints/
â””â”€â”€ exp3_batch_size/        # ì‹¤í—˜ 3
    â””â”€â”€ checkpoints/
```

```bash
# ëª¨ë“  ì‹¤í—˜ì´ ë™ì¼í•œ í† í¬ë‚˜ì´ì € ì‚¬ìš©
python demo_wmt14_saveable.py --load_dir artifacts_37k --checkpoint_dir exp1_baseline
python demo_wmt14_saveable.py --load_dir artifacts_37k --checkpoint_dir exp2_lr_tuning
python demo_wmt14_saveable.py --load_dir artifacts_37k --checkpoint_dir exp3_batch_size
```

## ìš”ì•½

### ì£¼ìš” ì¥ì 

1. **ì‹œê°„ ì ˆì•½:** 8-12ì‹œê°„ â†’ 3ë¶„ (99% ì ˆê°)
2. **ì¬í˜„ì„±:** ë™ì¼í•œ í† í¬ë‚˜ì´ì €ë¡œ ì—¬ëŸ¬ ì‹¤í—˜
3. **ìœ ì—°ì„±:** ë‹¤ì–‘í•œ vocab í¬ê¸° ì‚¬ì „ ì¤€ë¹„
4. **í¸ì˜ì„±:** ëª…ë ¹í–‰ ì˜µì…˜ìœ¼ë¡œ ê°„í¸ ì œì–´

### ê¶Œì¥ ì‚¬ìš©ë²•

1. **ì²˜ìŒ:** `--save_dir`ë¡œ ì €ì¥
2. **ì´í›„:** `--load_dir`ë¡œ ì¦‰ì‹œ ì‹œì‘
3. **ì‹¤í—˜:** ë‹¤ì–‘í•œ ì˜µì…˜ ì¡°í•©

ì´ì œ í† í¬ë‚˜ì´ì € í•™ìŠµì„ í•œ ë²ˆë§Œ í•˜ê³ , ë¬´í•œ ë°˜ë³µ ì‹¤í—˜ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤! ğŸš€
