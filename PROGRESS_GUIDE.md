# μ§„ν–‰λ¥  ν‘μ‹ λ²„μ „ μ‚¬μ© κ°€μ΄λ“

## νμΌ μ„¤λ…

### 1. tokenizer_with_progress.py
- BPE/WordPiece ν† ν¬λ‚μ΄μ € (μ§„ν–‰λ¥  ν‘μ‹ μ¶”κ°€)
- κΈ°μ΅΄ tokenizer.pyλ¥Ό λ€μ²΄
- λ‹¤μ μ •λ³΄λ¥Ό μ‹¤μ‹κ°„ ν‘μ‹:
  - λ‹¨μ–΄ λΉλ„ κ³„μ‚° μ§„ν–‰λ¥ 
  - BPE λ³‘ν•© μ§„ν–‰λ¥ 
  - μμƒ μ™„λ£ μ‹κ°„ (ETA)
  - λ³‘ν•© μ†λ„ (merges/s)

### 2. demo_wmt14_with_progress.py
- μ „μ²΄ WMT14 λ°μ΄ν„°μ…‹ ν•™μµ (μ§„ν–‰λ¥  ν‘μ‹ μ¶”κ°€)
- 4.5M λ¬Έμ¥ μ μ „μ²΄ μ‚¬μ©
- vocab_size=37000 (λ…Όλ¬Έ μ¤€μ)
- λ‹¤μ μ •λ³΄λ¥Ό μ‹¤μ‹κ°„ ν‘μ‹:
  - λ°°μΉ μ²λ¦¬ μ§„ν–‰λ¥ 
  - ν‰κ·  μ†μ‹¤
  - ν•™μµλ¥ 
  - λ°°μΉ μ²λ¦¬ μ†λ„
  - μμƒ μ™„λ£ μ‹κ°„

### 3. demo_quick_test.py
- λΉ λ¥Έ ν…μ¤νΈμ© μ¶•μ† λ²„μ „
- 10,000 λ¬Έμ¥λ§ μ‚¬μ© (μ „μ²΄μ 0.2%)
- vocab_size=5000
- 3 μ—ν­λ§ ν•™μµ
- μ „μ²΄ νμ΄ν”„λΌμΈ ν…μ¤νΈμ©

## μ‚¬μ© λ°©λ²•

### Option 1: λΉ λ¥Έ ν…μ¤νΈ (κ¶μ¥: μ²μ μ‹¤ν–‰ μ‹)

```bash
# 1. ν•„μ ν¨ν‚¤μ§€ μ„¤μΉ
pip install torch datasets transformers

# 2. λΉ λ¥Έ ν…μ¤νΈ μ‹¤ν–‰ (μ•½ 10-20λ¶„ μ†μ”)
python demo_quick_test.py
```

**μμƒ μ†μ” μ‹κ°„:**
- λ°μ΄ν„° λ΅λ“: ~30μ΄
- μμ–΄ ν† ν¬λ‚μ΄μ € ν›λ ¨: ~2λ¶„
- λ…μΌμ–΄ ν† ν¬λ‚μ΄μ € ν›λ ¨: ~2λ¶„
- μ–΄ν κµ¬μ¶•: ~10μ΄
- λ¨λΈ ν•™μµ (3 μ—ν­): ~5λ¶„
- **μ΄ μ†μ” μ‹κ°„: μ•½ 10-15λ¶„**

### Option 2: μ „μ²΄ ν•™μµ (λ…Όλ¬Έ μ¬ν„)

```bash
# μ „μ²΄ WMT14 λ°μ΄ν„°μ…‹μΌλ΅ ν•™μµ (μ μ‹κ°„ ~ μμΌ μ†μ”)
python demo_wmt14_with_progress.py
```

**μμƒ μ†μ” μ‹κ°„:**
- λ°μ΄ν„° λ΅λ“: ~2λ¶„
- μμ–΄ ν† ν¬λ‚μ΄μ € ν›λ ¨: **~4-6μ‹κ°„** (4.5M λ¬Έμ¥, vocab=37k)
- λ…μΌμ–΄ ν† ν¬λ‚μ΄μ € ν›λ ¨: **~4-6μ‹κ°„**
- μ–΄ν κµ¬μ¶•: ~5λ¶„
- λ¨λΈ ν•™μµ (100 μ—ν­): **μμΌ**
- **μ΄ μ†μ” μ‹κ°„: μ•½ 1-2μ£Ό** (GPU μ„±λ¥μ— λ”°λΌ λ‹¤λ¦„)

## μ§„ν–‰λ¥  μ¶λ ¥ μμ‹

### 1. ν† ν¬λ‚μ΄μ € ν›λ ¨ μ§„ν–‰λ¥ 

```
================================================================================
BPE Tokenizer Training Started
================================================================================
Corpus size: 4,508,785 sentences
Target vocab size: 37,000

[Step 1/4] Computing word frequencies...
  Progress: 4,508,785/4,508,785 (100.0%) | Elapsed: 123.5s | Done!
  Unique words: 845,231

[Step 2/4] Extracting alphabet...
  Alphabet size: 312
  Initial vocab size: 313

[Step 3/4] Splitting words into characters...
  Done!

[Step 4/4] Performing BPE merges...
  Target merges: 36,687
  Merges: 36,687/36,687 (100.0%) | Vocab: 37,000/37,000 | Speed: 45 merges/s | ETA: 0.0s | Done!

================================================================================
Training Complete!
  Final vocab size: 37,000
  Total merges: 36,687
  Total time: 1234.5s (20.6 min)
================================================================================
```

### 2. λ¨λΈ ν•™μµ μ§„ν–‰λ¥ 

```
================================================================================
Epoch 1/100
================================================================================
  Batch 5,400/5,412 (99.8%) | Loss: 3.2145 (avg: 4.5231) | LR: 0.000234 | Speed: 2.1 batch/s | ETA: 6s

  β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€
  Epoch 1 Summary:
    Train Loss: 4.5231 | Train PPL: 92.15
    Val Loss:   4.2341 | Val PPL:   68.92
    Epoch Time: 2571.3s (42.9 min)
    Total Time: 0.71 hours
  β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€

  β“ Checkpoint saved: saved/model_epoch_10.pt
```

## μ£Όμ” κ°μ„  μ‚¬ν•­

### 1. μ‹¤μ‹κ°„ μ§„ν–‰λ¥  ν‘μ‹

**Before:**
```
Training BPE tokenizers...
(λ‡ μ‹κ°„ λ™μ• μ•„λ¬΄ μ¶λ ¥ μ—†μ)
```

**After:**
```
[Step 4/4] Performing BPE merges...
  Merges: 15,234/36,687 (41.5%) | Vocab: 15,547/37,000 | Speed: 45 merges/s | ETA: 476.3s
```

### 2. μƒμ„Έν• ν†µκ³„ μ •λ³΄

**μ¶”κ°€λ μ •λ³΄:**
- μ§„ν–‰λ¥  (%)
- ν„μ¬/μ „μ²΄ κ°μ
- μ²λ¦¬ μ†λ„
- μμƒ μ™„λ£ μ‹κ°„ (ETA)
- κ²½κ³Ό μ‹κ°„
- ν‰κ·  μ†μ‹¤
- ν„μ¬ ν•™μµλ¥ 

### 3. μ„Ήμ…λ³„ κµ¬λ¶„

κ° λ‹¨κ³„κ°€ λ…ν™•ν κµ¬λ¶„λμ–΄ ν„μ¬ μ–΄λ–¤ μ‘μ—…μ΄ μ§„ν–‰ μ¤‘μΈμ§€ μ‰½κ² νμ•… κ°€λ¥:
- λ°μ΄ν„°μ…‹ λ΅λ“
- ν† ν¬λ‚μ΄μ € ν›λ ¨
- μ–΄ν μ‚¬μ „ κµ¬μ¶•
- Iterator μƒμ„±
- λ¨λΈ μ΄κΈ°ν™”
- ν•™μµ μ§„ν–‰

## μ„±λ¥ μµμ ν™” ν

### 1. ν† ν¬λ‚μ΄μ € ν›λ ¨ μ†λ„ ν–¥μƒ

ν„μ¬ ν† ν¬λ‚μ΄μ € ν›λ ¨μ΄ κ°€μ¥ μ¤λ κ±Έλ¦½λ‹λ‹¤. λΉ λ¥΄κ² ν•λ ¤λ©΄:

**λ°©λ²• 1: μƒν”λ§**
```python
# μ „μ²΄ λ°μ΄ν„° λ€μ‹  μΌλ¶€λ§ μ‚¬μ©
sample_size = 500000  # 50λ§κ°λ§ μ‚¬μ©
en_corpus = [item['translation']['en'] for item in train[:sample_size]]
```

**λ°©λ²• 2: λ―Έλ¦¬ ν›λ ¨λ ν† ν¬λ‚μ΄μ € μ €μ¥/λ΅λ“**
```python
# ν›λ ¨ ν›„ μ €μ¥
import pickle
with open('bpe_en.pkl', 'wb') as f:
    pickle.dump(bpe_en, f)

# λ‹¤μ μ‹¤ν–‰ μ‹ λ΅λ“
with open('bpe_en.pkl', 'rb') as f:
    bpe_en = pickle.load(f)
```

### 2. λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ¤„μ΄κΈ°

**λ°©λ²• 1: max_tokens μ΅°μ •**
```python
# GPU λ©”λ¨λ¦¬μ— λ§κ² μ΅°μ •
train_iter, valid_iter, test_iter = loader.make_iter(
    train, valid, test,
    max_tokens=12000,  # 25000 β†’ 12000μΌλ΅ μ¤„μ„
    device='cuda'
)
```

**λ°©λ²• 2: λ¨λΈ ν¬κΈ° μ¶•μ†**
```python
model = Transformer(
    d_model=256,      # 512 β†’ 256
    ffn_hidden=1024,  # 2048 β†’ 1024
    n_head=4,         # 8 β†’ 4
    n_layers=4,       # 6 β†’ 4
    ...
)
```

### 3. ν•™μµ μ†λ„ ν–¥μƒ

**λ°©λ²• 1: Mixed Precision Training**
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    output = model(src, trg[:, :-1])
    loss = criterion(...)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**λ°©λ²• 2: Gradient Accumulation**
```python
accumulation_steps = 4

for batch_idx, batch in enumerate(train_iter):
    output = model(src, trg[:, :-1])
    loss = criterion(...) / accumulation_steps
    loss.backward()
    
    if (batch_idx + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

## λ¬Έμ  ν•΄κ²°

### Q1: ν† ν¬λ‚μ΄μ € ν›λ ¨μ΄ λ„λ¬΄ μ¤λ κ±Έλ¦Ό (4-6μ‹κ°„)

**A1: μƒν”λ§ μ‚¬μ©**
```python
# 50λ§κ°λ§ μ‚¬μ© (μ „μ²΄μ 11%)
en_corpus = [item['translation']['en'] for item in train[:500000]]
# μμƒ μ‹κ°„: μ•½ 40-60λ¶„
```

**A2: λ” μ‘μ€ vocab_size μ‚¬μ©**
```python
bpe_en = BPETokenizer(vocab_size=10000)  # 37000 β†’ 10000
# μμƒ μ‹κ°„: μ•½ 1-2μ‹κ°„
```

### Q2: "tokenizer.json: 100%" μ—μ„ λ©μ¶¤

**A:** μ΄κ²ƒμ€ λ©μ¶ κ²ƒμ΄ μ•„λ‹™λ‹λ‹¤. 
- GPT-2 ν† ν¬λ‚μ΄μ € λ‹¤μ΄λ΅λ“ μ™„λ£
- λ‹¤μ λ‹¨κ³„(μ½”νΌμ¤ μ²λ¦¬) μ§„ν–‰ μ¤‘
- μ§„ν–‰λ¥  ν‘μ‹ λ²„μ „μ€ μ΄ λ¶€λ¶„λ„ ν‘μ‹ν•©λ‹λ‹¤

### Q3: OOM (Out of Memory) μ—λ¬

**A1: max_tokens μ¤„μ΄κΈ°**
```python
max_tokens=12000  # 25000 β†’ 12000
```

**A2: batch_size λ€μ‹  max_tokens μ‚¬μ© ν™•μΈ**
```python
# β μλ»λ μ‚¬μ©
loader.make_iter(..., batch_size=128)  # κ³ μ • ν¬κΈ°

# β… μ¬λ°”λ¥Έ μ‚¬μ©
loader.make_iter(..., max_tokens=25000)  # ν† ν° κΈ°λ°
```

### Q4: ν•™μµμ΄ μ§„ν–‰λλ”μ§€ ν™•μΈν•κ³  μ‹¶μ

**A:** μ§„ν–‰λ¥  ν‘μ‹ λ²„μ „ μ‚¬μ©
```bash
python demo_wmt14_with_progress.py
```

μ¶λ ¥ μμ‹:
```
Batch 1,234/5,412 (22.8%) | Loss: 4.2341 | Speed: 2.1 batch/s | ETA: 1987s
```

## μ²΄ν¬ν¬μΈνΈ μ €μ¥

λ¨λΈμ€ 10 μ—ν­λ§λ‹¤ μλ™ μ €μ¥λ©λ‹λ‹¤:

```
saved/
  β”β”€β”€ model_epoch_10.pt
  β”β”€β”€ model_epoch_20.pt
  β”β”€β”€ model_epoch_30.pt
  β””β”€β”€ ...
```

κ° μ²΄ν¬ν¬μΈνΈλ” λ‹¤μ μ •λ³΄λ¥Ό ν¬ν•¨:
- λ¨λΈ κ°€μ¤‘μΉ
- Optimizer μƒνƒ
- Scheduler μƒνƒ
- μ—ν­ λ²νΈ
- ν•™μµ/κ²€μ¦ μ†μ‹¤

### μ²΄ν¬ν¬μΈνΈμ—μ„ μ¬κ°

```python
# μ²΄ν¬ν¬μΈνΈ λ΅λ“
checkpoint = torch.load('saved/model_epoch_50.pt')

model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
start_epoch = checkpoint['epoch']

# ν•™μµ μ¬κ°
for epoch in range(start_epoch, 100):
    # ...
```

## κ¶μ¥ μ›ν¬ν”λ΅μ°

### 1λ‹¨κ³„: λΉ λ¥Έ ν…μ¤νΈ (ν•„μ)
```bash
python demo_quick_test.py
```
- λ¨λ“  κ²ƒμ΄ μ •μƒ μ‘λ™ν•λ”μ§€ ν™•μΈ
- μ•½ 10-15λ¶„ μ†μ”

### 2λ‹¨κ³„: μ¤‘κ°„ κ·λ¨ ν…μ¤νΈ (κ¶μ¥)
```python
# demo_wmt14_with_progress.py μμ •
SAMPLE_SIZE = 100000  # 10λ§κ°λ§
VOCAB_SIZE = 10000
EPOCHS = 10
```
- μ•½ 2-3μ‹κ°„ μ†μ”
- μ „μ²΄ νμ΄ν”„λΌμΈ κ²€μ¦

### 3λ‹¨κ³„: μ „μ²΄ ν•™μµ (λ…Όλ¬Έ μ¬ν„)
```bash
python demo_wmt14_with_progress.py
```
- μμΌ μ†μ”
- GPU ν•„μ

## μμƒ μ†μ” μ‹κ°„ μ”μ•½

| μ‘μ—… | λΉ λ¥Έ ν…μ¤νΈ | μ¤‘κ°„ κ·λ¨ | μ „μ²΄ (λ…Όλ¬Έ) |
|------|------------|----------|------------|
| λ°μ΄ν„° ν¬κΈ° | 10K | 100K | 4.5M |
| Vocab ν¬κΈ° | 5K | 10K | 37K |
| ν† ν¬λ‚μ΄μ € | 4λ¶„ | 40λ¶„ | 8-12μ‹κ°„ |
| ν•™μµ (μ—ν­) | 3 | 10 | 100 |
| μ΄ μ†μ” μ‹κ°„ | 15λ¶„ | 3μ‹κ°„ | 1-2μ£Ό |

## λ§μ§€λ§‰ μ²΄ν¬λ¦¬μ¤νΈ

μ‹¤ν–‰ μ „ ν™•μΈ:
- [ ] GPU μ‚¬μ© κ°€λ¥ μ—¬λ¶€ ν™•μΈ
- [ ] μ¶©λ¶„ν• λ””μ¤ν¬ κ³µκ°„ (μµμ† 50GB)
- [ ] ν•„μ ν¨ν‚¤μ§€ μ„¤μΉ μ™„λ£
- [ ] λΉ λ¥Έ ν…μ¤νΈ λ¨Όμ € μ‹¤ν–‰
- [ ] μ¥μ‹κ°„ ν•™μµ μ‹ μ²΄ν¬ν¬μΈνΈ μ €μ¥ ν™•μΈ

μ¦κ±°μ΄ ν•™μµ λμ„Έμ”! π€
